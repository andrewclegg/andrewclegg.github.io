<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>The Plural of Anecdote</title><link href="http://www.andrewclegg.org/" rel="alternate"></link><link href="http://www.andrewclegg.org/feeds/news.atom.xml" rel="self"></link><id>http://www.andrewclegg.org/</id><updated>2015-06-06T00:00:00+01:00</updated><entry><title>Interview with a Data Scientist</title><link href="http://www.andrewclegg.org/news/PeadarCoyleInterview.html" rel="alternate"></link><updated>2015-06-06T00:00:00+01:00</updated><author><name>Andrew Clegg</name></author><id>tag:www.andrewclegg.org,2015-06-06:news/PeadarCoyleInterview.html</id><summary type="html">&lt;p&gt;&lt;em&gt;I was recently interviewed by Peadar Coyle for his data science blog, &lt;a href="https://peadarcoyle.wordpress.com/"&gt;Models are Illuminating and Wrong&lt;/a&gt;, as the latest in a series of features on people in the field. It&amp;#8217;s archived here for posterity, but I would recommend taking a look at the &lt;a href="https://peadarcoyle.wordpress.com/2015/05/14/interviews-with-data-scientists-the-collection/"&gt;whole collection&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. What project have you worked on do you wish you could go back to, and do&amp;nbsp;better?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The one that most springs to mind was an analytics and visualization platform called Palomino that my team at Pearson built: a custom &lt;span class="caps"&gt;JS&lt;/span&gt;/&lt;span class="caps"&gt;HTML5&lt;/span&gt; app on top of Elasticsearch, Hadoop and HBase, plus a bunch of other pipeline components, some open source and some in-house. It kind of worked, and we learnt a lot, but it was buggy, flaky at the scale we tried to push it to, and reliant on constant supervision. And it&amp;#8217;s no longer in use, mostly for those&amp;nbsp;reasons.&lt;/p&gt;
&lt;p&gt;It was pretty ambitious to begin with, but I got dazzled by shiny new toys and the lure of realtime intelligence, and brought in too many new bits of tech that there was no organisational support for. We discovered that distributed data stores and message queues are &lt;em&gt;never&lt;/em&gt; as robust as they claim (c.f. &lt;a href="https://aphyr.com/tags/jepsen"&gt;Jepsen&lt;/a&gt;); that most people don&amp;#8217;t really need realtime interactive analytics; and that supporting complex clustered applications (even internal ones) is really hard, especially in an organisation that doesn&amp;#8217;t really have a devops&amp;nbsp;culture.&lt;/p&gt;
&lt;p&gt;These days, I&amp;#8217;d try very hard to find a solution using existing tools &amp;#8212; &lt;a href="https://www.elastic.co/products/kibana"&gt;Kibana&lt;/a&gt; for example looks much more mature and powerful than it did when we started out, and has a whole community and coherent ecosystem around it. And I&amp;#8217;d definitely shoot for a much simpler architecture with fewer moving parts and unfamiliar components. Dan McKinley&amp;#8217;s article &lt;a href="http://mcfunley.com/choose-boring-technology"&gt;Choose Boring Technology&lt;/a&gt; is very relevant&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. What advice do you have to younger analytics professionals and in particular PhD students in the&amp;nbsp;Sciences?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I was asked this the other day by a recent PhD grad who was interested in a data science career, so I&amp;#8217;ll pass on what I told&amp;nbsp;him.&lt;/p&gt;
&lt;p&gt;I think there are broadly three kinds of work that take place under the general heading of &amp;#8220;data scientist&amp;#8221;, although, there are also plenty of exceptions to&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;The first is about turning data into business insight, via statistical modelling, forecasting, predictive analytics, customer segmentation and clustering, survival analysis, churn prediction, visualization, online experiment design, and selection or design of meaningful metrics and&amp;nbsp;KPIs.&lt;/p&gt;
&lt;p&gt;The second is about developing data-driven products and features for the web, e.g. recommendation engines, trend detectors, anomaly detectors, search and ranking engines, ad placement algorithms, spam and abuse classifiers, content fingerprinting and similarity scoring,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;The third is really a more modern take on what used to be called operational research, i.e. optimizing business processes algorithmically to reduce time or cost, or increase coverage or reported&amp;nbsp;satisfaction.&lt;/p&gt;
&lt;p&gt;In many companies these will be separate roles, and not all companies do all three. But you&amp;#8217;ll also see roles that involve two or occasionally all three of these, in varying proportions. I guess a good start is to think about which appeals to you the most, and that will help guide&amp;nbsp;you.&lt;/p&gt;
&lt;p&gt;Don&amp;#8217;t get confused by the nomenclature: &amp;#8220;data scientist&amp;#8221; could mean any of those things, or something else entirely that&amp;#8217;s been rebranded to look cool. And you could be doing any of those things and not be called a data scientist. Read the job specs closely and ask lots of&amp;nbsp;questions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. What do you wish you knew earlier about being a data&amp;nbsp;scientist?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, I wish I&amp;#8217;d taken double maths for A level, all those years ago! As it was, I took the single option, and chose the mechanics module over statistics, something that held me back ever since despite various post-graduate courses. There are certain things that are just harder to crowbar into an adult brain, if you don&amp;#8217;t internalize the concepts early enough. I think languages and music are in that category&amp;nbsp;too.&lt;/p&gt;
&lt;p&gt;(For our global readers: A-levels are the qualifications from the last two years of high school. You usually do three or four subjects, or at least you did in my day. You could do standard maths with mechanics or stats, or standard + further with both, which counted as two&amp;nbsp;qualifications.)&lt;/p&gt;
&lt;p&gt;I had a similar experience with biology &amp;#8212; I dropped it when I was 16 but ended up working in bioinformatics for several years. Statistics and biology are both subjects that are much more interesting than school makes them seem, and I wish I&amp;#8217;d known that at the&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. How do you respond when you hear the phrase &amp;#8216;big&amp;nbsp;data&amp;#8217;?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, I used to react with anger and contempt, and have given some &lt;a href="https://drive.google.com/file/d/0B1HztRme3ZjZZjA1WHFJY25lQnM/view"&gt;pretty opinionated talks&lt;/a&gt; on that subject before. It&amp;#8217;s one of those things you can&amp;#8217;t get away from in the enterprise &lt;span class="caps"&gt;IT&lt;/span&gt; world, but ironically, since I joined Etsy I&amp;#8217;ve been numbed to the phrase by over-exposure&amp;#8230; Just because the Github repo for our Scalding and Cascading code is called&amp;nbsp;&amp;#8220;BigData&amp;#8221;.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s a marketing term with very little information content &amp;#8212; rather like &amp;#8220;cloud&amp;#8221;. But unlike &amp;#8220;cloud&amp;#8221; I actually think it&amp;#8217;s actively misleading &amp;#8212; it focuses attention on the size aspect, when most organisations have interesting and potentially valuable datasets that can fit on a laptop, or at least a medium-sized server. For that matter, a server with a &lt;em&gt;terabyte&lt;/em&gt; of &lt;span class="caps"&gt;RAM&lt;/span&gt; isn&amp;#8217;t much over $20K these days. &amp;#8220;Big data&amp;#8221; makes &lt;span class="caps"&gt;IT&lt;/span&gt; departments go all weak-kneed with delight or terror at the prospect of getting a Hadoop (or Spark) cluster, even though that&amp;#8217;s often not the right fit at&amp;nbsp;all.&lt;/p&gt;
&lt;p&gt;And as a noun phrase, it sucks, as it really doesn&amp;#8217;t refer to anything. You can&amp;#8217;t say &amp;#8220;we solved this problem with big data&amp;#8221; as big data isn&amp;#8217;t really a thing with any consistent&amp;nbsp;definition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. What is the most exciting thing about your&amp;nbsp;field?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s an interesting one. Deep learning is huge right now, but part of me still suspects it&amp;#8217;s a passing fad, partly because I&amp;#8217;m old enough to remember when plain-old neural networks were at the same stage of the hype cycle. Then they fell by the wayside for years. That said, the concrete improvements shown by convolutional nets on image recognition tasks are pretty&amp;nbsp;impressive.&lt;/p&gt;
&lt;p&gt;Time will tell whether that feat can be replicated in other domains. Recent work on &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;recurrent nets for modelling sequences&lt;/a&gt; (text, music, etc.) is interesting, and there&amp;#8217;s been some fascinating work from Google (and their acquihires DeepMind) on learning to &lt;a href="http://arxiv.org/abs/1312.5602"&gt;play video games&lt;/a&gt; or &lt;a href="http://arxiv.org/abs/1410.4615"&gt;parse and execute code&lt;/a&gt;. These last two examples both combine deep learning with non-standard training methods (reinforcement learning and curriculum learning respectively), and my money&amp;#8217;s on this being the direction that will really shake things up. But I&amp;#8217;m a layman as far as this stuff&amp;nbsp;goes.&lt;/p&gt;
&lt;p&gt;One problem with neural architectures is that they&amp;#8217;re often black boxes, or at least pretty dark grey &amp;#8212; hard to interpret or gain much insight from. There are still a lot of huge domains where this is a hard sell, education and healthcare being good examples. Maybe someone will invent a learning method with the transparency of decision trees but the power of deep nets, and win over those people in jobs where &amp;#8220;just trust the machine&amp;#8221; doesn&amp;#8217;t&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. How do you go about framing a data problem - in particular, how do you avoid spending too long, how do you manage expectations etc. How do you know what is good&amp;nbsp;enough?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It took me a long time to realise this, but short release cycles with small iterative improvements are the way to go. &lt;em&gt;Any&lt;/em&gt; result that shows an improvement over your current baseline is a result &amp;#8212; so even if you think there are much bigger wins to be had, get it into production, and test it on real data, while you work on its replacement. (Or if you&amp;#8217;re in academia, get a quick workshop paper out while you work on its&amp;nbsp;replacement!)&lt;/p&gt;
&lt;p&gt;This is also a great way to avoid overfitting, especially if you are in industry, or a service-driven academic field like bioinformatics. Instead of constantly bashing away at the error rate on a well-worn standard test set, get some new data from actual users (or cultures or sensors or whatever) and see if your model holds up in real life. And make sure you&amp;#8217;re optimizing for the right thing &amp;#8212; i.e. that your evaluation metrics really reflect the true cost of a&amp;nbsp;misprediction.&lt;/p&gt;
&lt;p&gt;I worked in natural language processing for quite a while, and I&amp;#8217;m sure that field was held back for a while by collective, cultural overfitting to the same-old datasets, like Penn Treebank section 23. There&amp;#8217;s an &lt;a href="http://hunch.net/?p=22"&gt;old John Langford article&lt;/a&gt; about this and other non-obvious ways to overfit, which is always worth a&amp;nbsp;re-read.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://peadarcoyle.wordpress.com/2015/06/06/interview-with-a-data-scientist-andrew-clegg/"&gt;Original version at Peadar&amp;#8217;s&amp;nbsp;blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</summary><category term="interviews"></category><category term="data science"></category></entry><entry><title>Lies, Damned Lies and Dataviz</title><link href="http://www.andrewclegg.org/news/PearsonLabsBlogPost.html" rel="alternate"></link><updated>2014-06-03T00:00:00+01:00</updated><author><name>Andrew Clegg</name></author><id>tag:www.andrewclegg.org,2014-06-03:news/PearsonLabsBlogPost.html</id><summary type="html">&lt;p&gt;I recently gave a talk at the &lt;a href="http://theinnovationenterprise.com/summits/big-data-innovation-summit-london-2014/"&gt;Big Data Innovation Summit&lt;/a&gt; called &lt;a href="http://www.slideshare.net/AndrewClegg1/lies-damned-lies-dataviz"&gt;Lies, Damned Lies &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Dataviz: Bad visualization, and how to avoid it&lt;/a&gt;. The aim: educate and entertain. I&amp;#8217;m not above resorting to mockery when a graphic is particularly incompetent or manipulative, and I&amp;#8217;ve never had that many laughs from a conference crowd&amp;nbsp;before.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.slideshare.net/AndrewClegg1/lies-damned-lies-dataviz"&gt;&lt;img src="../images/political_example.png" alt="Politicians lie -- who knew" title="Politicians lie -- who knew" width="100%"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pearson Labs asked to write up a short blog post version which is &lt;a href="http://labs.pearson.com/lies-damned-lies-dataviz-when-data-visualization-goes-wrong/"&gt;here&lt;/a&gt; &amp;mdash; enjoy. And if you liked that, I&amp;#8217;ll be revisiting the same subject at a &lt;a href="https://skillsmatter.com/conferences/1959-big-data-exchange-2014#program"&gt;Skills Matter event in November&lt;/a&gt;.&lt;/p&gt;</summary><category term="visualization"></category><category term="events"></category></entry><entry><title>Introduction</title><link href="http://www.andrewclegg.org/news/Introduction.html" rel="alternate"></link><updated>2014-06-02T00:00:00+01:00</updated><author><name>Andrew Clegg</name></author><id>tag:www.andrewclegg.org,2014-06-02:news/Introduction.html</id><summary type="html">&lt;p&gt;Releasing &lt;a href="https://github.com/andrewclegg/snake-charmer"&gt;Snake Charmer&lt;/a&gt; has inspired me to start blogging again, so welcome to the new&amp;nbsp;blog.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve had a couple of Wordpress blogs in the past, but this time I&amp;#8217;ve decided to give &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; a go, for a bit more control. And because Markdown and&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;Pelican&amp;#8217;s basically a toolkit for rendering source documents (Markdown, reStructuredText, AsciiDoc) as static &lt;span class="caps"&gt;HTML&lt;/span&gt; pages, and generating all the usual blogging extras like index pages, category pages and so on. It&amp;#8217;s very flexible, hackable, scriptable and command-line-driven, which is ideal for me. So let&amp;#8217;s see how it&amp;nbsp;goes.&lt;/p&gt;
&lt;p&gt;A note on the blog title, in case you&amp;#8217;re interested. There&amp;#8217;s an &lt;a href="http://www.psychologytoday.com/blog/scientocracy/201401/the-plural-anecdote-is-not-data"&gt;often&lt;/a&gt;-&lt;a href="http://www.ft.com/cms/s/2/634fb176-bf3f-11e3-b924-00144feabdc0.html#axzz33YVGNMQM"&gt;quoted&lt;/a&gt; quotation that goes &amp;#8220;the plural of anecdote is not data,&amp;#8221; referring to the comparative credibility of different kinds of evidence. Or at least, many people seem to think that&amp;#8217;s how it goes &amp;mdash; but without any particular reason to believe so. Ironically, the original quote was &amp;#8220;&lt;a href="http://blog.revolutionanalytics.com/2011/04/the-plural-of-anecdote-is-data-after-all.html"&gt;the plural of anecdote &lt;em&gt;is&lt;/em&gt; data&lt;/a&gt;&amp;#8221; (my&amp;nbsp;emphasis).  &lt;/p&gt;</summary><category term="intro"></category><category term="test"></category><category term="pelican"></category></entry></feed>